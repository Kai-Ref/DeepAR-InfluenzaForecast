{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4c33fb-9ff2-40e6-aefd-5b36675a016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gluonts\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.rolling_dataset import generate_rolling_dataset, StepStrategy\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "os.chdir('/home/reffert/DeepAR_InfluenzaForecast')\n",
    "from PythonFiles.model import model, preprocessing, split_forecasts_by_week\n",
    "from PythonFiles.Configuration import Configuration\n",
    "config = Configuration()\n",
    "df = pd.read_csv(\"/home/reffert/DeepAR_InfluenzaForecast/Notebooks/DataProcessing/influenza.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f3a285-e109-47c0-8428-1ec1c6acb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = list(df.location.unique())\n",
    "\n",
    "# Process the df into a uniformly spaced df\n",
    "df = df.loc[df.location.isin(locations), [\"value\", 'location', 'date']]\n",
    "corrected_df = preprocessing(config, df, check_count=False, output_type=\"corrected_df\")\n",
    "\n",
    "# seperate the intervals for training and testing\n",
    "train_set_df = corrected_df.loc[(corrected_df.index <= config.train_end_time) &\n",
    "                          (corrected_df.index >= config.train_start_time),:]\n",
    "\n",
    "test_set_df = corrected_df.loc[(corrected_df.index >= config.train_start_time) &\n",
    "                         (corrected_df.index <= config.test_end_time),:]\n",
    "\n",
    "# Format the train and test_set into a PandasDataset\n",
    "train_set = PandasDataset.from_long_dataframe(dataframe=train_set_df,\n",
    "                                              item_id='location',\n",
    "                                              target=\"value\",\n",
    "                                              freq=config.freq)\n",
    "\n",
    "test_set = PandasDataset.from_long_dataframe(dataframe=test_set_df,\n",
    "                                             item_id='location',\n",
    "                                             target=\"value\",\n",
    "                                             freq=config.freq)\n",
    "\n",
    "# Determine the Starting and ending time of the test_set_df\n",
    "start_time = min(test_set_df.index.difference(train_set_df.index))\n",
    "end_time = max(test_set_df.index.difference(train_set_df.index))\n",
    "test_set = generate_rolling_dataset(dataset=test_set,\n",
    "                                    strategy = StepStrategy(prediction_length=4, step_size=1),\n",
    "                                    start_time = pd.Period(start_time, config.freq),\n",
    "                                    end_time = pd.Period(end_time, config.freq)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050a035-8dfa-44ca-8d85-7bf0389efa3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.70it/s, epoch=1/4, avg_epoch_loss=1.04]\n",
      "100%|██████████| 50/50 [00:19<00:00,  2.56it/s, epoch=2/4, avg_epoch_loss=0.9]  \n",
      "100%|██████████| 50/50 [00:16<00:00,  2.96it/s, epoch=3/4, avg_epoch_loss=0.83] \n",
      "100%|██████████| 50/50 [00:16<00:00,  3.12it/s, epoch=4/4, avg_epoch_loss=0.826]\n"
     ]
    }
   ],
   "source": [
    "forecasts, tss = model(config, train_set, test_set, config.deeparestimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b7014-2354-4552-aaaa-9a42e5d3edac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "windows_per_location = int(len(test_set) / len(locations))\n",
    "for location in locations[:1]:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    plt.title(f'{location}')\n",
    "    # first plot the time series as a whole (x-axis: Date, y-axis: influenza-values)\n",
    "    plt.plot((corrected_df.loc[(corrected_df['location'] == location) &\n",
    "                            (corrected_df.index <= config.test_end_time) &\n",
    "                            (corrected_df.index >= config.train_end_time)].index),\n",
    "             corrected_df.loc[(corrected_df['location'] == location) &\n",
    "                           (corrected_df.index <= config.test_end_time) &\n",
    "                           (corrected_df.index >= config.train_end_time), 'value'])\n",
    "    plt.grid(which=\"both\")\n",
    "    #define the colors to use for each different window\n",
    "    color = [\"g\", \"r\", \"purple\", \"black\", \"yellow\", \"grey\"] * windows_per_location\n",
    "    # Iterate through the correct index by changing the range with [windows_per_location]\n",
    "    for k in range(0 + windows_per_location*locations.index(location),\n",
    "                   windows_per_location + windows_per_location*locations.index(location)):\n",
    "        forecast_entry = forecasts[k]\n",
    "        prediction_intervals = (50.0, 90.0)\n",
    "        forecast_entry.plot(prediction_intervals=prediction_intervals, color=color[k % windows_per_location])\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ceb95-7a0b-4a30-99f2-98efdf53ef76",
   "metadata": {},
   "source": [
    "## Splitting the Forecasts into their week ahead composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6de343-239c-4156-9172-1cc12aeda8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_week_ahead_forecasts=[]\n",
    "for location in locations:\n",
    "    start_date_list = []\n",
    "    # define the index of the time wise first forecast point\n",
    "    first_time_point_of_location = windows_per_location + windows_per_location*locations.index(location)-1\n",
    "    # save the array corresponding to the first week \n",
    "    weekly_samples_array = forecasts[first_time_point_of_location].samples[:,:1]\n",
    "    # also append the time wise first [start_date] to [start_date_list]\n",
    "    start_date_list.append(forecasts[first_time_point_of_location].start_date)\n",
    "    for k in range(first_time_point_of_location - 1,\n",
    "                   first_time_point_of_location - windows_per_location, -1):\n",
    "        \n",
    "        weekly_samples_array = np.concatenate((weekly_samples_array, forecasts[k].samples[:, :1]), axis=1)\n",
    "        start_date_list.append(forecasts[k].start_date)\n",
    "    \n",
    "    one_week_ahead_forecasts.append(gluonts.model.forecast.SampleForecast(\n",
    "        info=forecasts[first_time_point_of_location].info,\n",
    "        item_id=forecasts[first_time_point_of_location].item_id,\n",
    "        samples=weekly_samples_array,\n",
    "        start_date=min(start_date_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d378b4-d2e4-4ba8-9e23-144814871f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLot the Splitted Forecasts for each location\n",
    "for location in locations[:1]:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    plt.title(f'{location}')\n",
    "    # first plot the time series as a whole (x-axis: Date, y-axis: influenza-values)\n",
    "    plt.plot((corrected_df.loc[(corrected_df['location'] == location) &\n",
    "                            (corrected_df.index <= config.test_end_time) &\n",
    "                            (corrected_df.index >= config.train_end_time)].index),\n",
    "             corrected_df.loc[(corrected_df['location'] == location) &\n",
    "                           (corrected_df.index <= config.test_end_time) &\n",
    "                           (corrected_df.index >= config.train_end_time), 'value'])\n",
    "    #define the colors to use for each different window\n",
    "    color = [\"g\", \"r\", \"purple\", \"black\", \"yellow\", \"grey\"]\n",
    "    forecast_entry = one_week_ahead_forecasts[locations.index(location)]\n",
    "    prediction_intervals = (50.0, 90.0)\n",
    "    forecast_entry.plot(prediction_intervals=prediction_intervals, color=color[k % windows_per_location])\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a164413d-5249-4572-af9b-f9646d14ce26",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the Implementation within model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b575b4-c1cf-4ed1-ae22-a89832cec250",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_week_ahead_forecasts, split_tss  = split_forecasts_by_week(config, forecasts, tss, locations, 2, equal_time_frame=True)\n",
    "one_week_ahead_forecasts, split_tss  = split_forecasts_by_week(config, forecasts, tss, locations, 1, equal_time_frame=True)\n",
    "three_week_ahead_forecasts, split_tss  = split_forecasts_by_week(config, forecasts, tss, locations, 3, equal_time_frame=True)\n",
    "four_week_ahead_forecasts, split_tss  = split_forecasts_by_week(config, forecasts, tss, locations, 4, equal_time_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96414341-ba1a-4562-b2bc-2ef3ab8c54a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_dict ={1 : one_week_ahead_forecasts,\n",
    "                2 : two_week_ahead_forecasts,\n",
    "                3 : three_week_ahead_forecasts,\n",
    "                4 : four_week_ahead_forecasts}\n",
    "for location in locations[:1]:\n",
    "    for forecast in forecast_dict.values():\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "        d = {key for key in forecast_dict if forecast_dict[key] == forecast}\n",
    "        plt.title(f'{location} - {d}')\n",
    "        # first plot the time series as a whole (x-axis: Date, y-axis: influenza-values)\n",
    "        plt.plot((corrected_df.loc[(corrected_df['location'] == location) &\n",
    "                                (corrected_df.index <= config.test_end_time) &\n",
    "                                (corrected_df.index >= config.train_end_time)].index),\n",
    "                 corrected_df.loc[(corrected_df['location'] == location) &\n",
    "                               (corrected_df.index <= config.test_end_time) &\n",
    "                               (corrected_df.index >= config.train_end_time), 'value'])\n",
    "        #define the colors to use for each different window\n",
    "        color = [\"g\", \"purple\", \"black\", \"grey\",\"yellow\"]*windows_per_location\n",
    "        forecast_entry = forecast[locations.index(location)]\n",
    "        prediction_intervals = (50.0, 90.0)\n",
    "        forecast_entry.plot(prediction_intervals=prediction_intervals, color=color[int(locations.index(location)) % windows_per_location])\n",
    "        plt.grid(which=\"both\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fc8f8-699a-4107-90a9-cfc32de237c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantiles=config.quantiles)\n",
    "evaluator_df2 = pd.DataFrame()\n",
    "for forecast in forecast_dict.values():\n",
    "    agg_metrics, item_metrics = evaluator(split_tss, forecast)\n",
    "    d = {key for key in forecast_dict if forecast_dict[key] == forecast}\n",
    "    for location in locations[:]:\n",
    "        item_metrics.loc[item_metrics.item_id == f\"{location}\", \"item_id\"] = f\"{location} {d}\"\n",
    "        evaluator_df2 = pd.concat([evaluator_df2, item_metrics[item_metrics.item_id == f\"{location} {d}\"]])\n",
    "    agg_metrics[\"item_id\"] = f\"aggregated {d}\"\n",
    "    evaluator_df2 = pd.concat([evaluator_df2, pd.DataFrame(agg_metrics,index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ccfc4-12bd-48da-b96d-62fc4ad9b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None,'display.max_columns', None,\n",
    "    'display.precision', 3,):\n",
    "    #print(evaluator_df[evaluator_df.item_id.isin([str(location) + \" {2}\" for location in locations[:10]])][['item_id', 'MSE']+[col for col in evaluator_df.columns if \"QuantileLoss\" in col]])\n",
    "    print(evaluator_df.loc[evaluator_df.mean_absolute_QuantileLoss>0,['item_id', 'MSE']+[col for col in evaluator_df.columns if \"Coverage\" in col]])\n",
    "    mean_weekly_quantile_loss = evaluator_df.loc[evaluator_df.mean_absolute_QuantileLoss >0,\"mean_absolute_QuantileLoss\"].mean()\n",
    "    print(mean_weekly_quantile_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826183f0-8821-4811-9bc7-fd97be9a7a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coverage(evaluator_df_dict):\n",
    "    week_coverage_dict = {}\n",
    "    coverage_columns = [col for col in evaluator_df_dict[list(evaluator_df_dict.keys())[0]].columns if \"Coverage\" in col]\n",
    "    coverage_columns.remove(\"MAE_Coverage\")\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(16, 9))\n",
    "    for week in range(1,5):\n",
    "        if week == 1:\n",
    "            axse = (0, 0)\n",
    "        if week == 2:\n",
    "            axse =(1, 0)\n",
    "        if week == 3:\n",
    "            axse = (0, 1)\n",
    "        if week == 4:\n",
    "            axse = (1, 1)\n",
    "        for key in evaluator_df_dict.keys():\n",
    "            week_coverage_dict[week] = evaluator_df_dict[key].loc[evaluator_df_dict[key].item_id.isin([\"aggregated {\"+ f\"{week}\" + \"}\"]), coverage_columns]\n",
    "            axs[axse].plot([0.0, 1.0], [0.0, 1.0])\n",
    "            axs[axse].scatter(config.quantiles, evaluator_df_dict[key].loc[evaluator_df_dict[key].item_id.isin([\"aggregated {\" + f\"{week}\" + \"}\"]), coverage_columns])\n",
    "            axs[axse].plot(config.quantiles, evaluator_df_dict[key].loc[evaluator_df_dict[key].item_id.isin([\"aggregated {\" + f\"{week}\" + \"}\"]), coverage_columns].T, label=f\"{key}. df\")\n",
    "            axs[axse].title.set_text(f\"{week}-Week Ahead Coverage\")\n",
    "            axs[axse].legend()\n",
    "plot_coverage({\"DeepAR 1\":evaluator_df, \"DeepAR 2\":evaluator_df2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f01121-73a0-458e-a7cf-2684e9399ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quantile_loss(evaluator_df, quantiles=[0.1, 0.9], week=1, number_of_locations=10):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    if type(number_of_locations) == int:\n",
    "        considered_locations = locations[:number_of_locations]\n",
    "    elif type(number_of_locations) == list:\n",
    "        considered_locations = number_of_locations\n",
    "    location_item_id_list = [str(location) + \" {\" + f\"{week}\"+\"}\" for location in considered_locations]\n",
    "    \n",
    "    ax.set_ylim(0,max(evaluator_df.loc[evaluator_df.item_id.isin(location_item_id_list),[col for col in evaluator_df.columns if \"QuantileLoss\" in col]].max())+100)\n",
    "    ax.set_ylabel(\"QuantileLoss\")\n",
    "    ax.set_xlabel(\"LocationIndex\")\n",
    "    for location in considered_locations:\n",
    "        plt.bar(location, evaluator_df.loc[evaluator_df.item_id.isin([str(location) + \" {\" + f\"{week}\"+\"}\"]),f'QuantileLoss[{quantiles[1]}]'] - \\\n",
    "                evaluator_df.loc[evaluator_df.item_id.isin([str(location) + \" {\" + f\"{week}\"+\"}\"]),f'QuantileLoss[{quantiles[0]}]'],\n",
    "                0.5,evaluator_df.loc[evaluator_df.item_id.isin([str(location) + \" {\" + f\"{week}\"+\"}\"]),f'QuantileLoss[{quantiles[0]}]'])\n",
    "    plt.scatter(considered_locations, evaluator_df.loc[evaluator_df.item_id.isin(location_item_id_list),f'QuantileLoss[{quantiles[0]}]'], c=\"black\", marker=\"^\", label=f\"QL {quantiles[0]}\")\n",
    "    plt.scatter(considered_locations, evaluator_df.loc[evaluator_df.item_id.isin(location_item_id_list),f'QuantileLoss[{quantiles[1]}]'], c=\"black\", marker=\"v\", label=f\"QL {quantiles[1]}\")\n",
    "    plt.scatter(considered_locations, evaluator_df.loc[evaluator_df.item_id.isin(location_item_id_list),'QuantileLoss[0.5]'],c=\"black\", label = \"QL 0.5\")\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Quantile Loss from {quantiles[0]} to {quantiles[1]} for the {week}-Week Ahead\")\n",
    "    plt.show()\n",
    "plot_quantile_loss(evaluator_df, quantiles=[0.1, 0.5], week=1, number_of_locations=10)\n",
    "plot_quantile_loss(evaluator_df, quantiles=[0.1, 0.5], week=4, number_of_locations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598438d1-cc49-454f-bd03-49409fefd5df",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameter training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386968f3-42c9-4dc5-a87a-f525d135b48f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gluonts.mx import Trainer\n",
    "df_dict = {}\n",
    "for context_length in [4, 52, 104]:\n",
    "    for num_layers in [2, 4, 8]:\n",
    "        for num_cells in [64, 128, 256]:\n",
    "            for epochs in [2, 4, 8]:                \n",
    "                config.context_length = context_length\n",
    "                config.num_layers = num_layers\n",
    "                config.num_cells = num_cells\n",
    "                config.trainer = Trainer(epochs=epochs)\n",
    "                identifier = f\"cl {context_length}, nl {num_layers}, nc {num_cells}, ep {epochs}\"\n",
    "                forecasts, tss = model(config, train_set, test_set, config.deeparestimator)\n",
    "                split_tss = split_forecasts_by_week(config, forecasts, tss, locations, 1, equal_time_frame=True)[1]\n",
    "                forecast_dict ={1 : split_forecasts_by_week(config, forecasts, tss, locations, 1, equal_time_frame=True)[0],\n",
    "                                2 : split_forecasts_by_week(config, forecasts, tss, locations, 2, equal_time_frame=True)[0],\n",
    "                                3 : split_forecasts_by_week(config, forecasts, tss, locations, 3, equal_time_frame=True)[0],\n",
    "                                4 : split_forecasts_by_week(config, forecasts, tss, locations, 4, equal_time_frame=True)[0]}\n",
    "                evaluator = Evaluator(quantiles=config.quantiles)\n",
    "                evaluator_df = pd.DataFrame()\n",
    "                for forecast in forecast_dict.values():\n",
    "                    agg_metrics, item_metrics = evaluator(split_tss, forecast)\n",
    "                    d = {key for key in forecast_dict if forecast_dict[key] == forecast}\n",
    "                    for location in locations[3:4]:\n",
    "                        item_metrics.loc[item_metrics.item_id == f\"{location}\", \"item_id\"] = f\"{location} {d}\"\n",
    "                        evaluator_df = pd.concat([evaluator_df, item_metrics[item_metrics.item_id == f\"{location} {d}\"]])\n",
    "                    agg_metrics[\"item_id\"] = f\"aggregated {d}\"\n",
    "                    evaluator_df = pd.concat([evaluator_df, pd.DataFrame(agg_metrics,index=[0])])\n",
    "                df_dict[identifier] = evaluator_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f688ee-c23d-4302-bf0c-19ea84d78325",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_df=pd.DataFrame()\n",
    "for key in df_dict.keys():\n",
    "    agg_df = df_dict[key]\n",
    "    agg_df['param'] = str(key)\n",
    "    evaluator_df = pd.concat([evaluator_df, agg_df])\n",
    "with pd.option_context('display.max_rows', None,'display.max_columns', None, 'display.precision', 3,):\n",
    "    print(evaluator_df[evaluator_df.index == 0][['item_id', 'param', 'MSE', 'QuantileLoss[0.025]', 'QuantileLoss[0.1]', 'QuantileLoss[0.5]', 'QuantileLoss[0.9]']])\n",
    "evaluator_df.to_csv(\"Evaluation.csv\",sep=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST_PYTHON_KERNEL",
   "language": "python",
   "name": "test_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
